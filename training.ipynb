{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:52.238666Z",
     "iopub.status.busy": "2025-03-19T15:07:52.238317Z",
     "iopub.status.idle": "2025-03-19T15:07:58.984369Z",
     "shell.execute_reply": "2025-03-19T15:07:58.983578Z",
     "shell.execute_reply.started": "2025-03-19T15:07:52.238636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Kaggle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def is_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# Usage\n",
    "if is_kaggle():\n",
    "    print(\"Running on Kaggle\")\n",
    "    sys.path.append('/kaggle/input/urex-helperscripts')\n",
    "    sys.path.append('/kaggle/input/pop909-midis')\n",
    "    !pip install pretty_midi\n",
    "    !pip install miditok\n",
    "else:\n",
    "    print(\"Not running on Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:58.986193Z",
     "iopub.status.busy": "2025-03-19T15:07:58.985875Z",
     "iopub.status.idle": "2025-03-19T15:07:58.992062Z",
     "shell.execute_reply": "2025-03-19T15:07:58.991397Z",
     "shell.execute_reply.started": "2025-03-19T15:07:58.986158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch  # Ensure PyTorch is imported\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from miditok import REMI\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from vae import BetaVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:58.993800Z",
     "iopub.status.busy": "2025-03-19T15:07:58.993601Z",
     "iopub.status.idle": "2025-03-19T15:07:59.137257Z",
     "shell.execute_reply": "2025-03-19T15:07:59.136638Z",
     "shell.execute_reply.started": "2025-03-19T15:07:58.993783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcusongqy\u001b[0m (\u001b[33mmidi-vae\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Marcus\\Desktop\\UREX\\UREx_midivae\\wandb\\run-20250319_232205-kk78vum3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/midi-vae/midi-vae/runs/kk78vum3' target=\"_blank\">testing</a></strong> to <a href='https://wandb.ai/midi-vae/midi-vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/midi-vae/midi-vae' target=\"_blank\">https://wandb.ai/midi-vae/midi-vae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/midi-vae/midi-vae/runs/kk78vum3' target=\"_blank\">https://wandb.ai/midi-vae/midi-vae/runs/kk78vum3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_wandb = True\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    if is_kaggle():\n",
    "        # !pip install wandb\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n",
    "\n",
    "    else:\n",
    "        wandb_api_key = os.getenv(\"wandb_api_key\")\n",
    "    \n",
    "    wandb.login(key=wandb_api_key)\n",
    "        \n",
    "\n",
    "    wandb.init(\n",
    "        project=\"midi-vae\", \n",
    "        entity=\"midi-vae\", \n",
    "        name=\"testing\"  # Set your run name here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:59.138872Z",
     "iopub.status.busy": "2025-03-19T15:07:59.138616Z",
     "iopub.status.idle": "2025-03-19T15:07:59.144407Z",
     "shell.execute_reply": "2025-03-19T15:07:59.143675Z",
     "shell.execute_reply.started": "2025-03-19T15:07:59.138850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aux fns\n",
    "\n",
    "def midi_data_loader(folder, shuffle=True):\n",
    "    tokenizer = REMI()  # using defaults parameters\n",
    "    midi_paths = [path.resolve() for path in Path(folder).rglob(\"*.mid\")][:100] # to limit files actually used, for testing purposes\n",
    "\n",
    "    dataset = DatasetMIDI(\n",
    "        files_paths=midi_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=1024,\n",
    "        bos_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer[\"BOS_None\"],\n",
    "    )\n",
    "    collator = DataCollator(tokenizer.pad_token_id)\n",
    "    data_loader = DataLoader(dataset=dataset, collate_fn=collator, batch_size=32, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences to the same length\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    # Add a feature dimension\n",
    "    batch = batch.unsqueeze(-1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:59.145374Z",
     "iopub.status.busy": "2025-03-19T15:07:59.145168Z",
     "iopub.status.idle": "2025-03-19T15:07:59.180352Z",
     "shell.execute_reply": "2025-03-19T15:07:59.179657Z",
     "shell.execute_reply.started": "2025-03-19T15:07:59.145355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_dim = 128           # Each time step has 128 features (piano roll)\n",
    "hidden_dim = 256          # GRU hidden dimension\n",
    "latent_dim = 64           # Size of the latent space\n",
    "beta = 4.0                # Adjust beta for stronger disentanglement\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = BetaVAE(input_dim, hidden_dim, latent_dim, beta=beta)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "if use_wandb:\n",
    "    config_dict = {\n",
    "        \"input_dim\": input_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"beta\": beta,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,  # Store the optimizer type\n",
    "    }\n",
    "    # Update wandb config\n",
    "    wandb.config.update(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:07:59.181480Z",
     "iopub.status.busy": "2025-03-19T15:07:59.181152Z",
     "iopub.status.idle": "2025-03-19T15:08:13.142249Z",
     "shell.execute_reply": "2025-03-19T15:08:13.141507Z",
     "shell.execute_reply.started": "2025-03-19T15:07:59.181426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"dataset_train\"\n",
    "test_data_dir = \"dataset_test\"\n",
    "\n",
    "if is_kaggle():\n",
    "    train_data_dir = \"/kaggle/input/pop909-midis/\" + train_data_dir\n",
    "    test_data_dir = \"/kaggle/input/pop909-midis/\" + test_data_dir\n",
    "\n",
    "train_data_loader = midi_data_loader(train_data_dir, shuffle=True)\n",
    "test_data_loader = midi_data_loader(test_data_dir, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:08:13.143450Z",
     "iopub.status.busy": "2025-03-19T15:08:13.143148Z",
     "iopub.status.idle": "2025-03-19T15:08:13.154183Z",
     "shell.execute_reply": "2025-03-19T15:08:13.153521Z",
     "shell.execute_reply.started": "2025-03-19T15:08:13.143403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:08:13.156323Z",
     "iopub.status.busy": "2025-03-19T15:08:13.156093Z",
     "iopub.status.idle": "2025-03-19T15:08:13.172556Z",
     "shell.execute_reply": "2025-03-19T15:08:13.171830Z",
     "shell.execute_reply.started": "2025-03-19T15:08:13.156302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaVAE(\n",
       "  (encoder_gru): GRU(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc_mu): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc_logvar): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc_latent_to_hidden): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (decoder_gru): GRU(128, 256, num_layers=2, batch_first=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an embedding layer (vocab_size depends on your tokenizer)\n",
    "vocab_size = len(train_data_loader.dataset.tokenizer.vocab)\n",
    "embedding_layer = nn.Embedding(vocab_size, input_dim).to(device)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:08:13.173977Z",
     "iopub.status.busy": "2025-03-19T15:08:13.173683Z",
     "iopub.status.idle": "2025-03-19T15:08:25.449465Z",
     "shell.execute_reply": "2025-03-19T15:08:25.448486Z",
     "shell.execute_reply.started": "2025-03-19T15:08:13.173936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4/4 [00:29<00:00,  7.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 23.8699\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4/4 [00:33<00:00,  8.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 7.5967\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4/4 [00:29<00:00,  7.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 5.0029\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:27<00:00,  6.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 3.9621\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4/4 [00:26<00:00,  6.71s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.7086\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 4/4 [00:27<00:00,  6.91s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2318\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 4/4 [00:26<00:00,  6.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.8572\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 4/4 [00:27<00:00,  6.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.6368\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 4/4 [00:26<00:00,  6.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.4879\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 4/4 [00:27<00:00,  6.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = batch[\"input_ids\"].to(device)\n",
    "        embedded = embedding_layer(tokens.long())  # shape: (batch, seq_len, input_dim)\n",
    "        \n",
    "        # Forward pass through VAE\n",
    "        recon_x, mu, logvar = model(embedded.float())\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, _, _ = model.loss_function(recon_x, embedded.float(), mu, logvar)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        # Log the loss to wandb\n",
    "        wandb.log({\"epoch\": epoch + 1, \"avg_loss\": avg_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:08:25.450716Z",
     "iopub.status.busy": "2025-03-19T15:08:25.450477Z",
     "iopub.status.idle": "2025-03-19T15:08:25.474081Z",
     "shell.execute_reply": "2025-03-19T15:08:25.473456Z",
     "shell.execute_reply.started": "2025-03-19T15:08:25.450695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# saving the model at the end of training\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "if use_wandb and is_kaggle():\n",
    "    wandb.save(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:09:24.123684Z",
     "iopub.status.busy": "2025-03-19T15:09:24.123327Z",
     "iopub.status.idle": "2025-03-19T15:09:25.458893Z",
     "shell.execute_reply": "2025-03-19T15:09:25.458099Z",
     "shell.execute_reply.started": "2025-03-19T15:09:24.123656Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 4.8111\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_test_loss = 0.\n",
    "    for batch in test_data_loader:\n",
    "        tokens = batch[\"input_ids\"].to(device)\n",
    "        embedded = embedding_layer(tokens.long())  # (batch, seq_len, input_dim)\n",
    "\n",
    "        recon_x, mu, logvar = model(embedded.float())\n",
    "        loss, _, _ = model.loss_function(recon_x, embedded.float(), mu, logvar)\n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "    avg_test_loss = total_test_loss / len(test_data_loader)\n",
    "    print(f\"Average test loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\"average test loss\": avg_test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:09:29.038739Z",
     "iopub.status.busy": "2025-03-19T15:09:29.038394Z",
     "iopub.status.idle": "2025-03-19T15:09:31.079600Z",
     "shell.execute_reply": "2025-03-19T15:09:31.078952Z",
     "shell.execute_reply.started": "2025-03-19T15:09:29.038713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average test loss</td><td>▁</td></tr><tr><td>avg_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average test loss</td><td>4.8111</td></tr><tr><td>avg_loss</td><td>1.40999</td></tr><tr><td>epoch</td><td>10</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">testing</strong> at: <a href='https://wandb.ai/midi-vae/midi-vae/runs/kk78vum3' target=\"_blank\">https://wandb.ai/midi-vae/midi-vae/runs/kk78vum3</a><br> View project at: <a href='https://wandb.ai/midi-vae/midi-vae' target=\"_blank\">https://wandb.ai/midi-vae/midi-vae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250319_232205-kk78vum3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T15:09:34.585314Z",
     "iopub.status.busy": "2025-03-19T15:09:34.584996Z",
     "iopub.status.idle": "2025-03-19T15:09:34.682280Z",
     "shell.execute_reply": "2025-03-19T15:09:34.681381Z",
     "shell.execute_reply.started": "2025-03-19T15:09:34.585287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example function to generate from an existing tokenized MIDI file\n",
    "def generate_from_token_file(test_midi_file_path, test_output_file_path):\n",
    "    # Create a small dataset/loader from the single file\n",
    "    tokenizer = REMI()\n",
    "    single_dataset = DatasetMIDI(\n",
    "        files_paths=[Path(test_midi_file_path)],\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=1024,\n",
    "        bos_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer[\"BOS_None\"],\n",
    "    )\n",
    "    collator = DataCollator(tokenizer.pad_token_id)\n",
    "    single_loader = DataLoader(single_dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(single_loader))\n",
    "        tokens = batch[\"input_ids\"].to(device)  # shape: (1, seq_len)\n",
    "        \n",
    "        # Embed tokens\n",
    "        embedded = embedding_layer(tokens.long())  # shape: (1, seq_len, input_dim)\n",
    "        \n",
    "        # Encode to latent\n",
    "        mu, logvar = model.encode(embedded.float())\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode back to feature vectors\n",
    "        decoded = model.decode(z, seq_len=256)  # pick a sequence length\n",
    "        predicted_tokens = torch.argmax(decoded, dim=-1)  # shape: (1, seq_len)\n",
    "\n",
    "        # convert predicted tokens to a plain Python list, so that __ids_to_tokens can read it\n",
    "        predicted_tokens = predicted_tokens.squeeze().tolist()\n",
    "        \n",
    "        # Convert integers to token strings\n",
    "        token_strings = tokenizer._ids_to_tokens(predicted_tokens)\n",
    "        # Convert token strings back to MIDI\n",
    "        generated_midi = tokenizer([token_strings])\n",
    "        # print(len(tokens))\n",
    "        generated_midi.dump_midi(Path(test_output_file_path))\n",
    "\n",
    "if is_kaggle():\n",
    "    test_midi_file_path = \"/kaggle/input/pop909-midis/dataset_valid/001_t0_0.mid\"\n",
    "    test_output_file_path = \"/kaggle/working/trained_decoded_estimate.mid\"\n",
    "else:\n",
    "    test_midi_file_path = \"dataset_valid/001_t0_0.mid\"\n",
    "    test_output_file_path = \"trained_decoded_estimate.mid\"\n",
    "\n",
    "generate_from_token_file(test_midi_file_path, test_output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6813299,
     "sourceId": 11089335,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6813208,
     "sourceId": 11089801,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "midivae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
